version: '3.9'
services:
  whisper-speech:
    build:
      context: ./whisper
      dockerfile: Dockerfile
    # network_mode: host
    volumes:
      - ./whisper/:/app/ # Change me
    ports:
      - "3348:3348"
    command: [ "python", "app.py" ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
  fish-speech:
    build:
      context: ./fish-speech
      dockerfile: Dockerfile
    # network_mode: host
    volumes:
      - ./fish-speech/data/:/opt/fish-speech/references/ # Change me
      - ./fish-speech/api.py:/opt/fish-speech/api.py # Change me /opt/fish-speech
    ports:
      - "22311:22311"
    command: uvicorn 'api:app' --workers 3 --host 0.0.0.0 --port 22311 
    # [ "python", "-m", "tools.api", "--listen", "0.0.0.0:22311", "--llama-checkpoint-path", "checkpoints/fish-speech-1.4", "--decoder-checkpoint-path", "checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth", "--decoder-config-name", "firefly_gan_vq", "--compile" ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
  tabbyapi:
    container_name: tabbyapi
    build:
      context: ./tabbyAPI-personal-fork
      dockerfile: ./docker/Dockerfile
      args:
        DO_PULL: "true"
    ports:
      - "5000:5000"
    environment:
      NAME: TabbyAPI
      NVIDIA_VISIBLE_DEVICES: all
    volumes:
      - /mnt/nas/llama3s/:/app/models
      - ./tabbyapi/config.yml:/app/config.yml
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
